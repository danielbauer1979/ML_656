{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module5_Tutorial_NonLinearModeling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Non-linear Regression Techniques\n",
        "\n",
        "Daniel Bauer, 2022\n",
        "\n",
        "In this tutorial, we will get acquainted with non-linear models.  In particular, we introduce a number of non-linear regression techniques -- including polynomial regression, regression splines, smoothing splines, and local regression -- in a simple example setting.  "
      ],
      "metadata": {
        "id": "JNGZbbCiRkR0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some of the techniques we introduce don't exist in the packages that come with Colab. So we will install two additional libraries: scikit-lego for local regression."
      ],
      "metadata": {
        "id": "6xa9J22nR22l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-lego"
      ],
      "metadata": {
        "id": "nE9d5Kpg7Az7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With that, let's install the relevat libraries."
      ],
      "metadata": {
        "id": "DvC0N_VKSFaw"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F5yZYbDPGZBq"
      },
      "source": [
        "import matplotlib.pyplot as plt \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.preprocessing import SplineTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklego.linear_model import LowessRegression\n",
        "\n",
        "import statsmodels.api as sm"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And clone our git repository so as to have access to the data."
      ],
      "metadata": {
        "id": "2PRV7QtLSLCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7rMHNGajOvV"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Non-linear Regression Models\n",
        "\n",
        "### Primer on Non-linear Regression Techniques\n",
        "\n",
        "Non-linear models expand on the more foundational models that we discussed before, particularly linear regression.  We consider three different though related categories of approaches, which are relatively straightforward in the context of a single feature, i.e. if there is only one $x$.\n",
        "\n",
        "1. Use *transformations* of $x$: A traditional approach that falls in this category is *polynomial regression*, where we simply add powers of the feature $x$ -- i.e. $x^2$, $x^3$, etc. -- to the regression problem.  However, polynomials have some limitations, particularly in extremal areas, because they fit the regression function globally.  Instead, in *spline regression*, piecewise polynomials that are only fit between *knots* are used, but one imposes restrictions such that the fit is still continuous and smooth.  With so-called *natural splines*, a linear function is used in the extremal (corner) areas so as to avoid erratic behavior when extrapolating.  Depending on the number of knots, the function can mimic more or less arbitrary shapes.\n",
        "\n",
        "2. Using an arbitrary function but *penalizing* said function for variation.  This yields a so-called *smoothing spline* (so the approach is related to 1.), but it also depends on a smoothing parameters that governs how much variation is allowed.  Similar to regularized methods, this parameter can be used to control *model complexity*.\n",
        "\n",
        "3. *Local regression*: Instead of using all points for predicting at a given point $x_0$, we run a *local* regression where we put more weight on the data points that are close to $x_0$ and less weight on data points that are far away.  The weighting is typically done via a so-called *kernel* function (generalized bell curve) so this is also referred to as *kernel regression*.\n"
      ],
      "metadata": {
        "id": "Ilfi7DvlSXsy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example in the context of height-weight relationship\n",
        "\n",
        "We use a straightforward example: Regressing the weight of people on their heights. The relevant dataset is taken from a [well known example dataset from Davis (1990)](https://rdrr.io/cran/carData/man/Davis.html):"
      ],
      "metadata": {
        "id": "iL_1gPeUSg9v"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcUhqqpbGZBr"
      },
      "source": [
        "hwdata2 = pd.read_csv('ML_656/Davis.csv', index_col=0) \n",
        "hwdata = hwdata2.sort_values('height')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look:"
      ],
      "metadata": {
        "id": "JoeN5uvIT6a-"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQcuj4taGZBs"
      },
      "source": [
        "hwdata2.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's defined the $x$ and $y$ vectors:"
      ],
      "metadata": {
        "id": "EHWdL7HEUJwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = hwdata2[['height']]\n",
        "y = hwdata2['weight']"
      ],
      "metadata": {
        "id": "zENkLwAFUOmA"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's start with polynomial regression, where we are taking advantage of so-called *model pipelines* within scikit-learn, where we \"pipe\" the regression model into our linear regression model:"
      ],
      "metadata": {
        "id": "QN6VXn_OT8Uv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orzorYDJjuTO"
      },
      "source": [
        "polynomial_regression = Pipeline([('poly', PolynomialFeatures(degree=4)),('linear', LinearRegression(fit_intercept=False))])\n",
        "polynomial_regression = polynomial_regression.fit(X, y)\n",
        "polynomial_regression.named_steps['linear'].coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize via a scatter plot with polynomial regression line:"
      ],
      "metadata": {
        "id": "i4vks-QWUfpO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7bk_XF8GZBt"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_poly = polynomial_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='g', label='polynomial regression df=4')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we may be overfitting a bit...\n",
        "\n",
        "Let's look at regression splines, again using a model pipeline (we get splines via 'SplineTransformer'):"
      ],
      "metadata": {
        "id": "vJ4RsocQUoEd"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2thG-dVqZFt"
      },
      "source": [
        "spline_regression = Pipeline([('splines', SplineTransformer(degree=2, n_knots=3, extrapolation='linear')),('linear', LinearRegression(fit_intercept=False))])\n",
        "spline_regression = spline_regression.fit(X, y)\n",
        "spline_regression.named_steps['linear'].coef_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a spline regression line:"
      ],
      "metadata": {
        "id": "gdHCaKuLU91L"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8Vh7s5qvNfg"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_splines = spline_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_splines, color='g', label='Spline regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "Finally, let's run a local regression, where we rely on 'LowessRegression' from scikit-lego:"
      ],
      "metadata": {
        "id": "4Llw3Eg6VGY7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mIdO1yQYv6gN"
      },
      "source": [
        "local_regression = LowessRegression(sigma=50).fit(X,y)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, let's visualize via a scatter plot with a regression line:"
      ],
      "metadata": {
        "id": "hlt2l-X-VR8b"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5WlCKnxSHL"
      },
      "source": [
        "height_grid = np.arange(hwdata.height.min(), hwdata.height.max()).reshape(-1,1)\n",
        "pred_local = local_regression.predict(hwdata[['height']])\n",
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_local, color='g', label='local regression')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looks neat.\n",
        "\n",
        "So the moral of the story is that these techniques present very neat tools for modeling non-linear relationships. \n",
        "\n",
        "Let's summarize by plotting altogether:"
      ],
      "metadata": {
        "id": "YML989SFVZZN"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znbTtt-dGZBx"
      },
      "source": [
        "plt.figure(figsize=(15,6))\n",
        "plt.scatter(hwdata.height, hwdata.weight, facecolor='None', edgecolor='k', alpha=0.3)\n",
        "plt.plot(hwdata.height, pred_poly, color='r', label='polynomial regression df=4')\n",
        "plt.plot(hwdata.height, pred_splines, color='b', label='Natural spline df=4')\n",
        "plt.plot(hwdata.height, pred_local, color='y', label='Local regression')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}