{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Assignment3_LASSO_Prompt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Assignment 3"
      ],
      "metadata": {
        "id": "vUjV7JbXHmjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's install relevant packages. Again, we're going to rely on the statistical learning toolkit ski-cit learn, which provides LASSO regression but also has many other predictive models. As before, it is less comfortable to use than some of the other packages and, unlike R, does not support formulas. But it is versatile and fast, and therefore one of the most popular predictive modeling toolkits."
      ],
      "metadata": {
        "id": "cKPJ-NiKInO2"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ignevWfjW8Vq"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression, Lasso, LassoCV\n",
        "from sklearn.metrics import mean_squared_error"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practical Application: Predicting Claim Sizes\n",
        "\n",
        "We consider a data set of claim sizes (severities) from Allstate, that was used in a [Kaggle competition](https://www.kaggle.com/c/allstate-claims-severity) and is now available from various repositories, e.g. [here](https://github.com/Architectshwet/Allstate-Claims-Severity-Data/blob/master/Datasets).\n",
        "\n",
        "Let's load it, and take a look:"
      ],
      "metadata": {
        "id": "E4SOe62bIt8g"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-CKUbcD-ZKHh"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNLJ23WBZVhK"
      },
      "source": [
        "dat_1 = pd.read_csv('ML_656/Allstate_train1.csv')\n",
        "dat_2 = pd.read_csv('ML_656/Allstate_train2.csv')\n",
        "dat_3 = pd.read_csv('ML_656/Allstate_train3.csv')\n",
        "df = pd.concat([dat_1,dat_2,dat_3])"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8198XlahW8Vs"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OEOBwID9b_Gg"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJwUTJVIbqdx"
      },
      "source": [
        "df.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it is a large data set, and it is particularly large in the $p$ direction -- that is, there are many co-variates. So possibly shrinkage and selection will come in handy here.  One quick comment about the dataset and many Kaggle competitions more generally:  We don't really know what the variables `catx' and 'contx' stand for, so it is difficult to use experience/intuition in building a model -- which is an important aspect in real-world applications.  So performing well in a kaggle competition does not necessarily qualify a data scientist to work in the insurance industry."
      ],
      "metadata": {
        "id": "IXlm-EAtJfYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing the data\n",
        "\n",
        "There are a few very small losses that are outliers.  We thus disregard losses that are very small and  keep only records with loss greater or equal to $\\$100$, also because we are interested in these in actual settings."
      ],
      "metadata": {
        "id": "evPFU8RwJpLA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohBtee79W8Vt"
      },
      "source": [
        "df = df[df['loss']>100]"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We delete the id column"
      ],
      "metadata": {
        "id": "p1MAtijztDvZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "del df['id']"
      ],
      "metadata": {
        "id": "supqlGG2tCkI"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since running the LASSO with the full set of data can take a very long time (especially if the penalty parameters aren's chosen right), it is fine to only us a sample of the data:"
      ],
      "metadata": {
        "id": "tfnPgI1LtHcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.sample(n=50000, random_state=45)"
      ],
      "metadata": {
        "id": "zova-nv9tZ-7"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We convert categoricals into dummies:"
      ],
      "metadata": {
        "id": "qdXL1JySKASA"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2M_UC8hdW8Vu"
      },
      "source": [
        "objects = []\n",
        "for c in df.columns:\n",
        "    if str(df[c].dtype) == 'object':\n",
        "        objects.append(c)\n",
        "X_ = df.drop(objects, axis = 1).astype('float64')\n",
        "X_ = X_.drop(['loss'], axis = 1)\n",
        "dummies = pd.get_dummies(df[objects], drop_first=True)\n",
        "X = pd.concat([X_, dummies], axis = 1)\n",
        "y = df.loss"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at out features:"
      ],
      "metadata": {
        "id": "gZjiAnGRKMw4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Qt-7U7W8Vv"
      },
      "source": [
        "X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(X.columns,X.std(axis=0))"
      ],
      "metadata": {
        "id": "ZUS-B_FTtxes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it seems like all the variables are already at similar scales, so it doesn't seem necessary to normalize the data.\n",
        "\n",
        "We still carry out this step just to make sure."
      ],
      "metadata": {
        "id": "NaYpKqwitunH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_org = X\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "plt.bar(X_org.columns,X.std(axis=0))"
      ],
      "metadata": {
        "id": "0lPwRLtht9CS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that now the standard deviations of all features are the same at 1."
      ],
      "metadata": {
        "id": "ko06MWeUt9pw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We split data into training and test sets:"
      ],
      "metadata": {
        "id": "YgiRVq6NKQRV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_XIc50uW8Vw"
      },
      "source": [
        "X_train, X_test , y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_test,y_test,test_size = 0.5, random_state=2)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We go to logs. The log-transformation makes the data much more amenable to regression."
      ],
      "metadata": {
        "id": "_aRwof99KfX4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = np.log(y_train)\n",
        "y_val = np.log(y_val)\n",
        "y_test = np.log(y_test)"
      ],
      "metadata": {
        "id": "OTOtlbO2KbC0"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Up to you**:\n",
        "- Run a basic OLS model for loss prediction.\n",
        "- Check if a LASSO regression can improve upon the basic OLS appproach, and how drastic the improvement will be. Proceed as follows:\n",
        "  - Run a LASSO regression.\n",
        "  - Evaluate and visualize the LASSO fit for a selection of tuning parameters.\n",
        "  - Determine a good choice for the tuning parameter.\n",
        "  - Evaluate the performance of your predictive model."
      ],
      "metadata": {
        "id": "qo7kJP_Q3r_f"
      }
    }
  ]
}