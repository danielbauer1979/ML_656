{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5Zc+NMzvLlKL1gw3iMdL7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/SLC_AlgFairness.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Session 9: Algorithmic Bias and Fairness\n",
        "\n",
        "Dani Bauer (w/ Jim Guszcza), 11/2023"
      ],
      "metadata": {
        "id": "Ccw0SuBI5LYt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial, we discuss approaches how to analyze a predictive algorithm with regards to \"fairness.\" We do so in the context of a well-known case study on an algorithm that assists judges in parole decisions. We go over different notions of fairness, discuss tradeoffs, and explain the intuition behind the results of the analyses. We also discuss approaches how to adjust algorithms to enforce fairness. A second case study, which we will ask you to work on, illustrates how these ideas apply in the actuarial context.\n",
        "\n",
        "### Load required packages"
      ],
      "metadata": {
        "id": "zpUgxOcS5NiJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kSv8hx144fah"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from statsmodels.graphics.mosaicplot import mosaic\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, classification_report, precision_score, roc_curve, auc"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aequitas\n",
        "#Another library that seems to be popular is fariness 360:\n",
        "#!pip install aif360"
      ],
      "metadata": {
        "id": "BrPUeW1L6Xa1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from aequitas.group import Group\n",
        "from aequitas.bias import Bias\n",
        "from aequitas.fairness import Fairness\n",
        "import aequitas.plot as ap"
      ],
      "metadata": {
        "id": "EsxbkyTO6by5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compas Case Study\n",
        "\n",
        "The Compas data originates from a [well-known case study on algorithmic bias](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing). The background is [as follows](https://www.propublica.org/article/how-we-analyzed-the-compas-recidivism-algorithm): Across the nation, judges, probation and parole officers are increasingly using algorithms to assess a criminal defendant's likelihood of becoming a recidivist---a term used to describe criminals who re-offend. One of the commercial tools made by Northpointe, Inc. is called COMPAS (which stands for Correctional Offender Management Profiling for Alternative Sanctions). The case study compares outcomes and risk scores for individuals belonging to different races. In what follows, we will go through some of these analyses ourselves.\n",
        "\n",
        "## The Compas Data\n",
        "\n",
        "The data is in our github folder. Let's take a look:"
      ],
      "metadata": {
        "id": "YB2Fhuab_xpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "metadata": {
        "id": "XjM0xW_pATMq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat = pd.read_csv('ML_656/compasdata.csv')\n",
        "dat.head()"
      ],
      "metadata": {
        "id": "2HhtisuQAi3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dat.describe()"
      ],
      "metadata": {
        "id": "sR0ZFQweBLV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data contains information on recidivism of 6,172 individuals as well as information on the individual's age, sex, criminal history, their ethnicity---and the risk score they received from the COMPAS algorithm.\n",
        "\n",
        "To simplify the situation, we focus on two ethnicity levels only: African-Americans and Caucasians."
      ],
      "metadata": {
        "id": "4hYinYNND79l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = dat.loc[(dat['ethnicity'] == 'Caucasian') | (dat['ethnicity'] == 'African_American')]\n",
        "dat.describe()"
      ],
      "metadata": {
        "id": "iE06ij2sD-vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we still have the majority of individuals included. We will consider the African-Americans as the \"protected\" class.\n",
        "\n",
        "We commence by exploring the data some and looking at fairness manually, but then we will also explore how to use the Aequitas package in this setting."
      ],
      "metadata": {
        "id": "M696IiNCEyOu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fairness scores vs. recidivism rates\n",
        "\n",
        "Let's start by comparing the COMPAS scores between the two ethic groups (see also the density plots from the fairness package above):"
      ],
      "metadata": {
        "id": "sM_2m9OIGRM2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.title(\"Score distribution by group\")\n",
        "plt.xlabel(\"Group: 1 = Caucasian, 2 = African-American\")\n",
        "plt.boxplot([dat.loc[dat['ethnicity'] == 'Caucasian']['probability'],dat.loc[dat['ethnicity'] == 'African_American']['probability']])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WH65Ol1QE2X-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of scores in the African-American group has a higher median and higher percentiles than the scores in the Caucasian group.\n",
        "\n",
        "However, consider the number of re-offenders between the group, we see the following:"
      ],
      "metadata": {
        "id": "1cW5E765ON6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "by_race = sns.countplot(x=\"ethnicity\", hue=\"Two_yr_Recidivism\", data=dat[dat.ethnicity.isin(['African_American', 'Caucasian'])], palette=aq_palette)"
      ],
      "metadata": {
        "id": "pMMz2A__PzNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider raw averages of re-offenders in the two groups -- for caucasians:"
      ],
      "metadata": {
        "id": "-Agc5sMDP3If"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "qI7OQvUNOqoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And for the protected group:"
      ],
      "metadata": {
        "id": "NnnHTO18PRS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.average(dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == 'yes')"
      ],
      "metadata": {
        "id": "W7denUplOgP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we can interpret the higher average COMPAS scores as resulting from statistical facts in this particular population.\n",
        "\n",
        "Let’s investigate accuracy, i.e., how accurate the algorithm is, on aggregate and by protected vs. non-protected class. In doing so, we determine the correctly classified re-offenders (true positives), the correctly cassified non-re-offenders (true negatives), the incorrectly classified re-offenders (false negatives), and the incorrectly classified non-re-offenders (false positives)—by class."
      ],
      "metadata": {
        "id": "e7LIq6T2PT3d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "TP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * dat.loc[dat['ethnicity'] == 'African_American']['predicted'])\n",
        "FP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "FP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * dat.loc[dat['ethnicity'] == 'African_American']['predicted'])\n",
        "FN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "FN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * (dat.loc[dat['ethnicity'] == 'African_American']['predicted'] == 0))\n",
        "TN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "TN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * (dat.loc[dat['ethnicity'] == 'African_American']['predicted'] == 0))"
      ],
      "metadata": {
        "id": "E-VVrFQk9AC9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"Accuracy\" now is simply the correctly classified individuals divided by all individuals."
      ],
      "metadata": {
        "id": "qiMjSDz-_9i-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = (TP_0+TP_1+TN_0+TN_1)/(TP_0 + FP_0 + TP_1 + FP_1 + TN_0 + FN_0 + TN_1 + FN_1)\n",
        "acc"
      ],
      "metadata": {
        "id": "l-YDztCj_Yb9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and by group:"
      ],
      "metadata": {
        "id": "919XS_aH_-jE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 = (TP_0+TN_0)/(TP_0 + FP_0 + TN_0 + FN_0)\n",
        "acc_0"
      ],
      "metadata": {
        "id": "FYJy06F8ABDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "and for the protected group:"
      ],
      "metadata": {
        "id": "B7eWfGckAa74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_1 = (TP_1+TN_1)/(TP_1 + FP_1 + TN_1 + FN_1)\n",
        "acc_1"
      ],
      "metadata": {
        "id": "XITH9iGiAege"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(['Caucasian','African-American'], [acc_0,acc_1], color ='maroon', width = 0.4)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy:    (TP+TN) / (TP+FP+TN+FN)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CPxs79d8AykW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that the algorithm is similarly \"accurate\" for both groups, and in fact the accuracy is even somewhat higher for the protected group.\n",
        "\n",
        "Hence, one may conclude that the algorithm performs reasonably well---and that there are no major concerns with regards to differential performance for the protected group. In fact, this may have been the mindset by the creators of the COMPAS algorithm.\n",
        "\n",
        "### Alternative Fairness Metrics\n",
        "\n",
        "While \"accuracy\"---or one minus accuracy, which is the misclassification rate---do not point to problems, we can investigate more differentiated error metrics. One obvious way of doing this is by comparing\n",
        "\n",
        "We can further assess the performance by visualizing false negative rates and the true positive rates by group. One way of looking at this is how many of the negatively classified individuals (low risk) were labeled so incorrectly, and how many of the positively classified (high risk) were labeled so correctly. The former proportion of negatively classified for which outcome indeed was one is also referred to as the *False Omission Rate*, whereas the latter proportion of positively classified that were indeed correct is also referred to as *Precision*:"
      ],
      "metadata": {
        "id": "hqa9ZKM2Advy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['False Omission Rate',FN_0/(TN_0+FN_0),'Caucasian'],['False Omission Rate',FN_1/(TN_1+FN_1),'African_American'],['Precision',TP_0/(TP_0+FP_0),'Caucasian'],['Precision',TP_1/(TP_1+FP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette)"
      ],
      "metadata": {
        "id": "yBztwEiXFvu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start seeing some differences between the groups, although the differences are not startling. The algorithm seems to me more “precise” for the protected class.\n",
        "\n",
        "Differences between the groups start to emerge if we consider indviduals that are labeled as being likely to re-offend, i.e., the total fraction of positives between the two groups:"
      ],
      "metadata": {
        "id": "2OuRMyVaHrrV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['Proportional Parity:  (FP+TP) / (FP+TP+TN+FN)',(FP_0+TP_0)/(TN_0+FN_0+FP_0+TP_0),'Caucasian'],['Proportional Parity:  (FP+TP) / (FP+TP+TN+FN)',(FP_1+TP_1)/(TN_1+FN_1+FP_1+TP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette).set(title='proportion of group classified positive -- a measure of disparate impact')"
      ],
      "metadata": {
        "id": "IolCgsJhAzvM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, it appears that the COMPAS algorithm does have disparate impact between the two classes. Based on our comparisons from above, one may argue that this is a consequence of the statistical fact that within this population, the rate of re-offenders in the protected class was higher. Indeed, the disparate impact is associated with the accuracy of the algorithm.\n",
        "\n",
        "In relation to the disparate impact, we can ask how the algorithm performs with regards to accurately labeling individuals. We start by investigating true positives, i.e., the proportion of positives that were classified correctly—which is also called the sensitivity. One minus the sensitivity corresponds to the positives that were incorrectly labeled, i.e., the false negative rate:"
      ],
      "metadata": {
        "id": "SDSjVgRtCSQA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['Sensitivity (aka TPR):   TP / (TP + FN)',TP_0 / (FN_0+TP_0),'Caucasian'],['Sensitivity (aka TPR):   TP / (TP + FN)',TP_1 / (FN_1+TP_1),'African_American'],['False negative rate:  FN/(FN+TP)',FN_0 / (FN_0+TP_0),'Caucasian'],['False negative rate:  FN/(FN+TP)',FN_1 / (FN_1+TP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette)"
      ],
      "metadata": {
        "id": "c-DKesLxCS_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The relationship between the sensitivity and the false negative rate is:"
      ],
      "metadata": {
        "id": "ZERGR5hLC_F6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TP_0 / (FN_0+TP_0) + FN_0 / (FN_0+TP_0)"
      ],
      "metadata": {
        "id": "-JAnZDMMDE_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_1 / (FN_1+TP_1) + FN_1 / (FN_1+TP_1)"
      ],
      "metadata": {
        "id": "I-IyAKQqDLBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it appears that the COMPAS algorithm is more \"sensitive\" for the protected class, and, ergo, the false negative rate is higher for the non-protected class. The latter means that the the number of high-risk individuals that were missed---and erroneously recommended for parole---is higher for the non-protected class."
      ],
      "metadata": {
        "id": "jKOl1WkEDfa9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that *Sensitivity* is on the y-axis of the ROC curve, so we know there is a tradeoff between sensitivity and the *False Positive Rate* (FPR), which in turn is one minus *Specificity*. The FPR simply is the percent of negatives incorrecly labeled as one:"
      ],
      "metadata": {
        "id": "ClkscRClDmwT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['False positive rate:  FP/(FP+TN)',FP_0 / (FP_0+TN_0),'Caucasian'],['False positive rate:  FP/(FP+TN)',FP_1 / (FP_1+TN_1),'African_American'],['Specificity:   TN / (TN + FP)',TN_0 / (TN_0+FP_0),'Caucasian'],['Specificity:   TN / (TN + FP)',TN_1 / (TN_1+FP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette)"
      ],
      "metadata": {
        "id": "4ia8rJmmDl_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The relationship between the specificity and the false positive rate is:"
      ],
      "metadata": {
        "id": "79Gec1lDEeR5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FP_0 / (FP_0+TN_0) + TN_0 / (TN_0+FP_0)"
      ],
      "metadata": {
        "id": "X43F_172Eips"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FP_1 / (FP_1+TN_1) + TN_1 / (TN_1+FP_1)"
      ],
      "metadata": {
        "id": "X3CjH28KEic6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We notice that the FPR is substantially higher for the protected class, i.e., the proportion of individuals that were erroneously labeled as high risk to re-offend is substantially higher in the protected class than in the non-protected class! Being more at risk of being erroneously labeled as high-risk to re-offend based on the color of risk does seem like a problem.\n",
        "\n",
        "### Intution: Generalized Confusion Matrix\n",
        "\n",
        "But how is it possible that the algorithm appears similarly accurate for the two groups, yet the chance of being erroneously classified as high risk is much higher for the protected group?\n",
        "\n",
        "To provide intuition, let's generate a generalized confusion table, where we illustrate all the correctly and erronrously classified individuals by class:\n"
      ],
      "metadata": {
        "id": "1LR1urLOEbdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_tmp = [['prot=0 event=0',TN_0,FP_0],['prot=0 event=1',FN_0,TP_0],['prot=1 event=0',TN_1,FP_1],['prot=1 event=1',FN_1,TP_1]]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=[' ', 'decision=0','decision=1'])\n",
        "df_tmp"
      ],
      "metadata": {
        "id": "7A6yymn8GiiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let’s visualize:"
      ],
      "metadata": {
        "id": "M96Tho04IB9N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = ['decision=0','decision=1']\n",
        "y1 = np.array([FN_1,TP_1])\n",
        "y2 = np.array([TN_1,FP_1])\n",
        "y3 = np.array([FN_0,TP_0])\n",
        "y4 = np.array([TN_0,FP_0])\n",
        "\n",
        "plt.bar(x, y1, color='r')\n",
        "plt.bar(x, y2, bottom=y1, color='b')\n",
        "plt.bar(x, y3, bottom=y1+y2, color='y')\n",
        "plt.bar(x, y4, bottom=y1+y2+y3, color='g')\n",
        "plt.ylabel(\"Count\")\n",
        "plt.legend([\"prot=1 event=1\", \"prot=1 event=0\", \"prot=0 event=1\", \"prot=0 event=0\"])\n",
        "plt.title(\"Scores by Teams in 4 Rounds\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gdmvrAsTMjdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have the labels on the horizontal, and the actual outcomes on the vertical. The large FPR is driven by the large fraction of individuals in the protected class labeled as one.\n",
        "\n",
        "### Enforcing Fairness via Different Thresholds\n",
        "\n",
        "As we illustrated, there are different notions of fairness one may apply. However, it seems intutive that the high FPR for the protected class may be perceived problematic. Being erroneously denied parole based on the color of one's skin arguably is perceived as unfair by many.\n",
        "\n",
        "This intuition is in line with prescriptions from scholars that have attempted to organize different fairness metrics. For instance, the bias audit toolkit [Aequitas](http://www.datasciencepublicpolicy.org/our-work/tools-guides/aequitas/) provides a \"fairness tree\" that organizes different metrics. Considering this situation where interventions are \"punitive\" and we are worried about individuals where \"intervention is not warranted\" (i.e., those that won't re-offend), Aequitas' fairness tree suggests that \"FPR Parity\"---i.e., equating FPR rates across the different classes---is an appropriate metric.\n",
        "\n",
        "How can we realize this? We can adjust the threshold for the protected class to above 0.5, i.e., requiring a higher score to be labeled high risk. After some experimentation, it appears 0.605 is a threshold that roughly equates false positive rates:"
      ],
      "metadata": {
        "id": "5crGyueUO7V_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "upd_afram = (dat.loc[dat['ethnicity'] == 'African_American']['probability'] > 0.605)"
      ],
      "metadata": {
        "id": "l6MI0oi-PYQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "TP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * upd_afram)\n",
        "FP_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'])\n",
        "FP_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * upd_afram)\n",
        "FN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"yes\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "FN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"yes\") * (upd_afram == 0))\n",
        "TN_0 = sum((dat.loc[dat['ethnicity'] == 'Caucasian']['Two_yr_Recidivism'] == \"no\") * (dat.loc[dat['ethnicity'] == 'Caucasian']['predicted'] == 0))\n",
        "TN_1 = sum((dat.loc[dat['ethnicity'] == 'African_American']['Two_yr_Recidivism'] == \"no\") * (upd_afram == 0))"
      ],
      "metadata": {
        "id": "p8INr9zCQez5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aq_palette = sns.diverging_palette(225, 35, n=2)\n",
        "data_tmp = [['False positive rate:  FP/(FP+TN)',FP_0 / (FP_0+TN_0),'Caucasian'],['False positive rate:  FP/(FP+TN)',FP_1 / (FP_1+TN_1),'African_American'],['Specificity:   TN / (TN + FP)',TN_0 / (TN_0+FP_0),'Caucasian'],['Specificity:   TN / (TN + FP)',TN_1 / (TN_1+FP_1),'African_American']]\n",
        "df_tmp = pd.DataFrame(data_tmp, columns=['Metric','rate','ethnicity'])\n",
        "sns.barplot(df_tmp, x=\"Metric\", y=\"rate\", hue=\"ethnicity\", palette=aq_palette)"
      ],
      "metadata": {
        "id": "c3ejCHwXQib7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adjusting the threshold generally results in a less \"accurate\" algorithm. Let's check the accuracy level for the modified algorithm:"
      ],
      "metadata": {
        "id": "UJOPpgn-SZUQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc = (TP_0+TP_1+TN_0+TN_1)/(TP_0 + FP_0 + TP_1 + FP_1 + TN_0 + FN_0 + TN_1 + FN_1)\n",
        "acc"
      ],
      "metadata": {
        "id": "Rvf53gR0SjSV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Surprisingly, the accuracy only drops slightly from the original accuracy score of 0.667.\n",
        "\n",
        "For the non-protected group it stays the same:"
      ],
      "metadata": {
        "id": "mGwwyMT5StUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_0 = (TP_0+TN_0)/(TP_0 + FP_0 + TN_0 + FN_0)\n",
        "acc_0"
      ],
      "metadata": {
        "id": "3SqMn_JOSnzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But there is a decrease for the protected class:"
      ],
      "metadata": {
        "id": "4X9kaOFDSw7H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc_1 = (TP_1+TN_1)/(TP_1 + FP_1 + TN_1 + FN_1)\n",
        "acc_1"
      ],
      "metadata": {
        "id": "4MigX4BqSriU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.bar(['Caucasian','African-American'], [acc_0,acc_1], color ='maroon', width = 0.4)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy:    (TP+TN) / (TP+FP+TN+FN)\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xEJwDyRTS5Bl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we obtain a slight drop for the protected group, but this appears a small price to pay to ensure the equivalence of false positive rates and the associated fairness in this example setting."
      ],
      "metadata": {
        "id": "B4ZGMM6uTD4Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using the Aequitas Library\n",
        "\n",
        "Let's now see how to use Aequitas library to analyze the predictions for fairness. More details can be found [here](https://colab.research.google.com/github/dssg/aequitas/blob/update_compas_notebook/docs/source/examples/compas_demo.ipynb).\n",
        "\n",
        "### Data Format\n",
        "\n",
        "Aequitas requires a column named `score` that has the predicted label by the algorithm (should be 0 / 1) and a column named `label_value` that has the actual class. We reformat our dataset to adhere to this format, and we also drop all the non-ethnicity columns to focus on that attribute. (One needs to drop the non-string columns in any case, otherwise there is an error message.)"
      ],
      "metadata": {
        "id": "OekxOdHmlHm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dat = dat.rename(columns={\"predicted\": \"score\", \"Two_yr_Recidivism\": \"label_value\"})\n",
        "dat = dat.drop(columns=['Number_of_Priors', 'probability','Unnamed: 0','Age_Above_FourtyFive','Age_Below_TwentyFive','Female','Misdemeanor'])\n",
        "dat.label_value = dat.label_value.map(dict(yes=1, no=0))\n",
        "dat.head()"
      ],
      "metadata": {
        "id": "W2uwPI-blH1I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Aequitas to Obtain Confusion Values and Rates\n",
        "\n",
        "We can now use Aequitas to obtain the count of true positives, false positives, true negatives, and false negatives by group--as well as corresponding rates."
      ],
      "metadata": {
        "id": "WUQ_rHiVsd-q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "g = Group()\n",
        "xtab, _ = g.get_crosstabs(dat)"
      ],
      "metadata": {
        "id": "Blh3SQmemExJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "absolute_metrics = g.list_absolute_metrics(xtab)"
      ],
      "metadata": {
        "id": "8Ih9ENEHnfXU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtab[[col for col in xtab.columns if col not in absolute_metrics]]"
      ],
      "metadata": {
        "id": "u1REfMgbnjtC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xtab[['attribute_name', 'attribute_value'] + absolute_metrics].round(2)"
      ],
      "metadata": {
        "id": "hfwnJnbzooy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bias Analyses"
      ],
      "metadata": {
        "id": "euOXY-KVtM-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now obtain bias outcomes, where we have to define a reference group. Here we use the Caucasian subgroup."
      ],
      "metadata": {
        "id": "JSVGUxsNtQoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "b = Bias()\n",
        "bdf = b.get_disparity_predefined_groups(xtab, original_df=dat,\n",
        "                                        ref_groups_dict={'ethnicity':'Caucasian'},\n",
        "                                        alpha=0.05, check_significance=True,\n",
        "                                        mask_significance=True)\n",
        "bdf.style"
      ],
      "metadata": {
        "id": "q6MS6HzzqbJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use a variety of metrics. Given the discussion above, we use false positive rate (FPR):"
      ],
      "metadata": {
        "id": "EAiatc3-texH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = ['fpr']\n",
        "disparity_tolerance = 1.25"
      ],
      "metadata": {
        "id": "8gMEIRpFq2iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's visualize:"
      ],
      "metadata": {
        "id": "QP2IBbNrtod8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ap.summary(bdf, metrics, fairness_threshold = disparity_tolerance)"
      ],
      "metadata": {
        "id": "lhfGKSN0q4ze"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ap.disparity(bdf, metrics, 'ethnicity', fairness_threshold = disparity_tolerance)"
      ],
      "metadata": {
        "id": "F9Gd1LwmrF-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have seen above, the FPR is substantially higher for the African-American poppulation--and also for native Americans."
      ],
      "metadata": {
        "id": "_XePvhIEts7v"
      }
    }
  ]
}