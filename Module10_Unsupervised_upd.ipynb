{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielbauer1979/ML_656/blob/main/Module10_Unsupervised_upd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unsupervised Learning: Clustering and PCA Analysis\n",
        "Dani Bauer, 2023\n",
        "\n",
        "As mentioned in the beginning of lecture, there are (at least) two basic learning setups.  In **supervised** machine learning one observes a response $Y$ with observing $p$ different features $X=(X_1,X_2,\\ldots,X_p)$, where we typically postulate the relationship $Y = f(X)+\\varepsilon$ and $\\varepsilon$ independent of $X$ with mean zero. Here quality is usually assessed by the (test/out-of-sample) error that compares predictions and realizations for a separate dataset.  In **unsupervised** learning, we only observe $p$ features $X_1,X_2,\\ldots,X_p$, and we would like to learn about their relationship -- without focussing on a supervising outcome.  Of course, the difficulty is how to assess quality in this case -- so different unsupervised learning techniques are very different, and which one to pick will depend on the nature of the problem.\n",
        "\n",
        "In this tutorial, we will take a closer look at two algorithms: **Principal Component Analysis (PCA)** and **Clustering**.  There are variety of other techniques, including anomaly detection, self-organizing maps, association analysis, etc. We start with quick introductions but then look into the applications of these techniques in more detail in the context of a case study.\n",
        "\n",
        "As usual, we start by implementing the relevant packages:"
      ],
      "metadata": {
        "id": "aIiejbOjljzy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vCC-SQb7VRR"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.lines as mlines\n",
        "import plotly.figure_factory as ff\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from random import sample\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler # For rescaling metrics to fit 0 to 1 range\n",
        "from sklearn.metrics import multilabel_confusion_matrix, confusion_matrix\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from sklearn.decomposition import PCA\n",
        "from scipy.spatial.distance import euclidean"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Principal Component Analysis\n",
        "\n",
        "## Background\n",
        "\n",
        "The key idea behind *Principal Component Analysis* (PCA) is to use \"meaningful\" linear combinations of the data:\n",
        "$$\n",
        "Z_m = \\sum_{j=1}^p \\phi_{jm} X_j \\text{ such that }\\sum_{j=1}^p \\phi^2_{jm} = 1,\n",
        "$$\n",
        "where the idea is to choose:\n",
        "\n",
        "- $\\phi_{j1},\\,j=1,\\ldots,p$  such that the the variance of $Z_1$ is maximal (i.e., so that it captures most of the variation in the $X$s)\n",
        "\n",
        "- $\\phi_{j2},\\,j=1,\\ldots,p$ such that the variance of $Z_2$ is maximial out of all the variables that are uncorrelated with $Z_1$.\n",
        "\n",
        "- $\\phi_{j3},\\,j=1,\\ldots,p$ such that the variance of $Z_3$ is maximial out of all the variables that are uncorrelated with $Z_1$ and $Z_2$. $\\ldots$\n",
        "\n",
        "Hence, one way to intepret the principal components are the linear combination that best reflect the variation in the data.\n",
        "\n",
        "Importantly, the scale of variables matters, so it is a good idea to center and scale your variables.  Also, the components are only determined up to their sign (plus/minus).  To discern how important different variables are, one typically considers the *Percent of Variance Explained* (PVE):\n",
        "$$\n",
        "\\text{PVE}_m = \\frac{\\sum_{i=1}^n \\left(\\sum_{j=1}^p \\phi_{jm}x_{ij}\\right)^2}{\\sum_{i=1}^n \\sum_{j=1}^p x_{ij}^2},\n",
        "$$\n",
        "and the resulting plot that depicts PVE by principal components is referred to as a *scee plot*.\n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a basic example, where we simulate heights and weights of a fictional population according to some arbitrary parameters.  More precisely, we assume that weight and height follow Normal distributions with a mean weight (in kg) of 70 and a mean height (in cm) of 170m, and variances of 25 and 150, respectively.  We assume the correlation parameter is 50%.\n"
      ],
      "metadata": {
        "id": "M0g7gj9NmeKj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mu = (70,170)\n",
        "cov = np.array([[25, np.sqrt(25)*np.sqrt(150)*0.5], [np.sqrt(25)*np.sqrt(150)*0.5, 150]])\n",
        "X_raw = np.random.multivariate_normal(mu, cov, size=5000)"
      ],
      "metadata": {
        "id": "WUPuLhokm73m"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check quick:"
      ],
      "metadata": {
        "id": "nwTiQwq_p0Fy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw.mean(axis=0)"
      ],
      "metadata": {
        "id": "bl3Q6vB_prhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.cov(np.transpose(X_raw))"
      ],
      "metadata": {
        "id": "NmFiKEcHqCjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.corrcoef(np.transpose(X_raw))"
      ],
      "metadata": {
        "id": "7CP5fiTOyaJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the *empirical* moments look similar to our theoretical moments. Let's take a peak:\n"
      ],
      "metadata": {
        "id": "8qFiIVu4p161"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw[:,0], X_raw[:,1], s = 1, color = 'black')\n",
        "plt.xlabel('weight')\n",
        "plt.ylabel('height')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "tY1m2LNRpBg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prinicipal components are closely related to the eigen-analysis (eigenvalues and eigenvectors) of the correlation (or covariance) matrix.  Let's illustrate (if you never had a linear algebra class, feel free to skip this part).  First, let's scale the data and calculate the (empirical) correlation matrix -- remember, in practical settings we usually don't know the underlying parameters:\n"
      ],
      "metadata": {
        "id": "_IXLO-52q0zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "R = np.corrcoef(np.transpose(X_raw))\n",
        "R"
      ],
      "metadata": {
        "id": "5VahnsYuq1xp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's calculate the eigenvalues and the eigenvectors of `R`.  As a reminder, the eigenvectors decompose a matrix (a.k.a. a linear mapping in finite dimensions) in orthogonal directions, whereas the eignvalues provide the \"length\" (importance) of the directions.  In particular, we can represent a symmetric matrix in diagnolized form by relying on eigenvectors and eigenvalues."
      ],
      "metadata": {
        "id": "mdNuryMYrJt6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Dec = np.linalg.eig(R)\n",
        "Dec"
      ],
      "metadata": {
        "id": "rpMC6B0drS6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = Dec[1] # Matrix of Eigenvectores\n",
        "D = np.diag(Dec[0]) # Diagonal Matrix of Eigenvalues\n",
        "np.dot(np.dot(V,D),np.transpose(V)) # Calculates V * D * V'"
      ],
      "metadata": {
        "id": "oGS9R-CxsVgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get intuition, let's plot the Eigenvectors:"
      ],
      "metadata": {
        "id": "yUlyHgUy3IyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(X_raw)\n",
        "plt.figure(figsize = (6,4))\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(scaler.transform(X_raw)[:,0], scaler.transform(X_raw)[:,1], s = 1, color = 'black')\n",
        "line1 = mlines.Line2D([0, 0.7071], [0, -.7071], color='red')\n",
        "line2 = mlines.Line2D([0, .7071], [0, .7071], color='yellow')\n",
        "transform = ax.transAxes\n",
        "ax.add_line(line1)\n",
        "ax.add_line(line2)\n",
        "plt.xlabel('scaled weight')\n",
        "plt.ylabel('scaled height')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_qNILkH44nBQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's determine the PCAs to compare:"
      ],
      "metadata": {
        "id": "NmDr6gNvDmW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca_fit = pca.fit(scaler.transform(X_raw))"
      ],
      "metadata": {
        "id": "mvQZVoseDD_n"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_fit.components_"
      ],
      "metadata": {
        "id": "9l6m5_ylDcLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is exactly this notion of \"importance\" that motivates principal components.  Indeed, the loadings of the principal components just amount to the *ordered* eigenvalues..."
      ],
      "metadata": {
        "id": "D100cSoIsTRT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering\n",
        "\n",
        "## Background\n",
        "\n",
        "*Clustering* refers to techniques for finding subgroups in a given dataset. The typical approach to determine clusters $C_1,\\ldots,C_K$ is to minimize:\n",
        "$$\n",
        "\\sum_{k=1}^K W(C_k),\n",
        "$$\n",
        "where $W$ is a measure of *variation* within a cluster.  For instance, **k-means clustering** uses the Euclidean distance to measure variation:\n",
        "$$\n",
        "W(C_k) = \\frac{1}{|C_k|} \\sum_{i,i' \\in C_k} \\sum_{j=1}^p (x_{ij} - x_{i'j})^2.\n",
        "$$\n",
        "The algorithms are implemented via a greedy algorithm by considering the centers of clusters (referred to as *centroids*).  The number of clusters $K$ must be chosen beforehand.  One approach is *hierarchical clustering*, where one starts with a larger number of clusters and then *fuses* custers that are similar (e.g., with regards to the distance between their centroids).\n",
        "\n",
        "## Simulated Example\n",
        "\n",
        "Let's consider a very basic simulated example -- let's simulate normal random variables with different means:"
      ],
      "metadata": {
        "id": "OEru_Pl4ACGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw2 = np.random.multivariate_normal((0,0), np.array([[1, 0], [0, 1]]), size=100)\n",
        "X_raw2[0:49,0]=X_raw2[1:50,0]+3\n",
        "X_raw2[0:49,1]=X_raw2[1:50,1]-4"
      ],
      "metadata": {
        "id": "tCFUcnLqbdpU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot:"
      ],
      "metadata": {
        "id": "Ozl7WSIvbdMs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (6,4))\n",
        "plt.scatter(X_raw2[0:49,0], X_raw2[0:49,1], color='red')\n",
        "plt.scatter(X_raw2[50:99,0], X_raw2[50:99,1], color='black')\n",
        "plt.xlabel('X0')\n",
        "plt.ylabel('X1')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AG2cFW5VctpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's run k means clustering:\n"
      ],
      "metadata": {
        "id": "5WdTAU69d6rp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans = KMeans(n_clusters = 2, init = 'k-means++', max_iter = 1000, random_state = 123)\n",
        "kmeans.fit(X_raw2)\n",
        "centroids = kmeans.cluster_centers_\n",
        "centroids"
      ],
      "metadata": {
        "id": "oq_YzcFwd6dY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcff6896-58ba-430e-c935-71022a9680b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.21092013,  0.05185911],\n",
              "       [ 2.89725937, -4.06564021]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label = kmeans.fit_predict(X_raw2)\n",
        "label"
      ],
      "metadata": {
        "id": "PhY9123eecf-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the algorithm was able to identify how we set up the data!"
      ],
      "metadata": {
        "id": "TPcmD4aOeq3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: County Health Rankings 2013\n",
        "\n",
        "We analyze [County Health Rankings](www.countyhealthrankings.org) in the US in 2013, based on a data set from the University of Wisconsin Population Health Institute.  I took this exammple from Jim Guszcza, and he put this together in R (it's much better than what I had before :-).\n",
        "\n",
        "## Data Preparation\n",
        "\n",
        "Let's load the data:"
      ],
      "metadata": {
        "id": "3RW20yqheke7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge_n_HRNgRm0"
      },
      "source": [
        "!git clone https://github.com/danielbauer1979/ML_656.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whMtWdEgtAh"
      },
      "source": [
        "health = pd.read_csv('ML_656/countyHealthRR.csv')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "V3w077i9fShr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's a large dataset, and the first three numbers are indices.  However, the remaning columns provide rather detailed information for each county.  Let's visualize:"
      ],
      "metadata": {
        "id": "W6DWnaeb0RHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(health.corr());"
      ],
      "metadata": {
        "id": "z8Ia-qwL0k1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So quite a bit of correlation, but nothing seems to be \"replicated.\"  \n",
        "\n",
        "Unfortunately, we have a bunch of missing data. Let's drop the columns where we have lots of missing values and then drop na-s:"
      ],
      "metadata": {
        "id": "4bfsz1CRe7y9"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "012bX41Lg9dF"
      },
      "source": [
        "health = health.drop(columns=['FIPS','State','County','Perc.Fair.Poor.Health','Perc.Smokers','Perc.Excessive.Drinking','MV.Mortality.Rate','Pr.Care.Physician.Ratio','Perc.No.Soc.Emo.Support'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nm0Hb3CxhytE"
      },
      "source": [
        "health = health.dropna()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the resulting data:"
      ],
      "metadata": {
        "id": "AOvjhvUr1Me8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health.info()"
      ],
      "metadata": {
        "id": "YHiF5HDW1PnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health.describe()"
      ],
      "metadata": {
        "id": "oeIESh-O1SwN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's scale the data so as to make sure we are comparing apples to apples:"
      ],
      "metadata": {
        "id": "T2-2bu_51cH1"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVSWGGQ-kVhg"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "scaler.fit(health)\n",
        "health_sc = scaler.transform(health)\n",
        "health_sc"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "### K-Means clustering\n",
        "\n",
        "Let's commence with k-means clustering, which is avalable in the library `sklearn.cluster` (along with other clustering algorithms, see below). To decide on the number of clusters, we evaluate how the within-sum-of-squares varies between the number of clusters using a so-called *elbow plot*:\n"
      ],
      "metadata": {
        "id": "VGmhd4rb1dlV"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF4Tvu-imXH7"
      },
      "source": [
        "wcss = []\n",
        "for i in range(2, 12):\n",
        "    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "    kmeans.fit(health_sc)\n",
        "    wcss.append(kmeans.inertia_)\n",
        "plt.bar(range(2, 12), wcss)\n",
        "plt.title('Elbow Method')\n",
        "plt.xlabel('Number of clusters')\n",
        "plt.ylabel('WCSS')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that six clusters seem to present a reasonable choice, there is a bit of an \"elbow\" there. So let's go with six:"
      ],
      "metadata": {
        "id": "LlIwaq1N3J4j"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0J1JGhuVnqGv"
      },
      "source": [
        "kmeans = KMeans(n_clusters=6, init='k-means++', max_iter=1000, n_init=10, random_state=0)\n",
        "kmeans.fit(health_sc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get the labels via:"
      ],
      "metadata": {
        "id": "cOXho1k23ok6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "kmeans.labels_"
      ],
      "metadata": {
        "id": "jtsoEKP_3tLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's add to our dataset:"
      ],
      "metadata": {
        "id": "fgE-k3VH9nLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmeans.labels_"
      ],
      "metadata": {
        "id": "GJ_sfTvh9pE2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the different clusters by evaluating the YPLL.Rate (Years of Potential Life Lost)."
      ],
      "metadata": {
        "id": "hHZYmnlK_Jrf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bG0zgwt5-XMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "HTplXWaf_eLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ddf2dbEl_eON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "QZPQh5Xc_eRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "kq_DhtSp_eTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_means_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "pNqE83nb_n-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's relabel so it's increasing:"
      ],
      "metadata": {
        "id": "euotX4dR_tFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relab(label):\n",
        "  if label == 0:\n",
        "    return 0\n",
        "  elif label == 1:\n",
        "    return 1\n",
        "  elif label == 2:\n",
        "    return 5\n",
        "  elif label == 3:\n",
        "    return 2\n",
        "  elif label == 4:\n",
        "    return 4\n",
        "  else:\n",
        "    return 3"
      ],
      "metadata": {
        "id": "Umgy4pQ-F7m7"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kmlab = list(map(relab, kmeans.labels_))"
      ],
      "metadata": {
        "id": "reVaZbJTHH08"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_means_cl'] = kmlab"
      ],
      "metadata": {
        "id": "LeY28w_K5hGM"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hierarchical Clustering\n",
        "\n",
        "Let's use the hierarchical clustering algorithm in scikit (`AgglomerativeClustering`). Plotting a *dendrogram* is easier with the `scipy.cluster.hierarchy`, yet given the size of the dataset we don't see a ton here.\n",
        "\n",
        "To compare with the k-means approach, let's run a hierarchical clustering also with six clusters."
      ],
      "metadata": {
        "id": "yWrU6Bnl3u0d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch = AgglomerativeClustering(n_clusters=6)\n",
        "hierarch.fit(health_sc)"
      ],
      "metadata": {
        "id": "1oBxQTqs7p4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Again, we can get the labels via:"
      ],
      "metadata": {
        "id": "gYDv_mNx8tTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hierarch.labels_"
      ],
      "metadata": {
        "id": "Ogxz292p8J9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also add to the dataset and carry out a similar approach as before:"
      ],
      "metadata": {
        "id": "Q3_hVGRN55AV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierarch.labels_"
      ],
      "metadata": {
        "id": "3mgy8-NK5-Ld"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 0]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "ib6Xuvyo8p4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 1]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "X9t8epBf8p1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 2]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "lpS1mFd18pyR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 3]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "bMakZhkB8pq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 4]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "d0sBEYLn8pgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health[health['k_hier_cl'] == 5]['YPLL.Rate'].mean()"
      ],
      "metadata": {
        "id": "Inm4wef88zpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def relab2(label):\n",
        "  if label == 0:\n",
        "    return 5\n",
        "  elif label == 1:\n",
        "    return 2\n",
        "  elif label == 2:\n",
        "    return 0\n",
        "  elif label == 3:\n",
        "    return 4\n",
        "  elif label == 4:\n",
        "    return 3\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "5cHreh5l9BK6"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hierlab = list(map(relab2, hierarch.labels_))"
      ],
      "metadata": {
        "id": "6rUa25gq9X_1"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health['k_hier_cl'] = hierlab"
      ],
      "metadata": {
        "id": "kgom-7Rl9hCS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's compare the two:"
      ],
      "metadata": {
        "id": "VzFnwZJW5yj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels = [0,1,2,3,4,5]\n",
        "confusion_matrix(health['k_means_cl'],health['k_hier_cl'],labels=labels)"
      ],
      "metadata": {
        "id": "u84XOdUS9qs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it looks like there is quite a bit of overlap, particularly with regards to clusters 0, 1, and 2. It appears clusters 4 and 5 are overlapping. However, it also is clear that assigning points to clusters isn't highly obvious."
      ],
      "metadata": {
        "id": "ps_F7hxy8JrU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualizing Clusters\n",
        "\n",
        "Let's visualize clusters by comparing the distributions of some of the features across clusters. As we had discussed, comparing distributions can be conveniently accomplished via box-whisker plots.\n",
        "\n",
        "Let's start with children in poverty."
      ],
      "metadata": {
        "id": "X7GF4qiTA0Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Children.in.Poverty\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "HYKSMgk6_wJE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we sorted the clusters according to YPLL.Rate (Years of Potential Life Lost), so more years of life lost is a sign of worse health. It appears that this weakly aligns with children poverty.\n",
        "\n",
        "Let's look at violent crimes."
      ],
      "metadata": {
        "id": "0CDlt4LMBRi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Violent.Crime.Rate\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BD-oXwLXBwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So here we have a weak alignment, although cluster 4 seems a bit lower. Let's look at fast food concentration."
      ],
      "metadata": {
        "id": "gypA3GKUB9JR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Fast.Foods\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yRJkARhMCQDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA\n",
        "\n",
        "Recall that the dataset contains 24 features. As we explained above, PCA tries to determine the most relevant \"directions\" to span this space. Here, a \"direction\" corresponds to some weighted average of the data. The largest principal component is the most important direction, the second largest principal component is the second most important direction, etc.\n",
        "\n",
        "We want to choose the number of principal components that accounts for a lot of variation in terms of \"explained variance\"."
      ],
      "metadata": {
        "id": "h2Gc5ZLnlUfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6,4))\n",
        "pca = PCA(n_components=10)\n",
        "pca_fit = pca.fit(health_sc)\n",
        "pc_values = np.arange(pca_fit.n_components_) + 1\n",
        "plt.bar(pc_values, pca.explained_variance_ratio_)\n",
        "plt.title('Principal Component Analysis Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LN-0NLihlfgx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hence, the first three components explain:"
      ],
      "metadata": {
        "id": "zN78TRPcmqKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca.explained_variance_ratio_[0]+pca.explained_variance_ratio_[1]+pca.explained_variance_ratio_[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y52hA9iumglH",
        "outputId": "55763468-7112-45f0-9534-4ecd069eb434"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5512169495805175"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "of the variance in the data. So while there is still a lot of other variation, the first three principal components explain a lot already.\n",
        "\n",
        "So let's work with those:"
      ],
      "metadata": {
        "id": "YISnNj9VmuAk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 4)\n",
        "principalComponents = pca.fit(health_sc)\n",
        "principalComponents.components_\n",
        "PCweights = pd.DataFrame(data = principalComponents.components_, columns = [\"YPLL.Rate\",\"Physically.Unhealthy.Days\",\"Mentally.Unhealthy.Days\",\"Perc.Low.Birth.Weight\",\"Perc.Obese\",\"Perc.Physically.Inactive\",\"Chlamydia.Rate\",\"Teen.Birth.Rate\",\"Perc.Uninsured\",\"Dentist.Ratio\",\"Prev.Hosp.Stay.Rate\",\"Perc.Diabetic.Screening\",\"Perc.Mammography\",\"Perc.High.School.Grad\",\"Perc.Some.College\",\"Perc.Unemployed\",\"Perc.Children.in.Poverty\",\"Perc.Single.Parent.HH\",\"Violent.Crime.Rate\",\"Avg.Daily.Particulates\",\"Perc.pop.in.viol\",\"Rec.Facility.Rate\",\"Perc.Limited.Access\",\"Perc.Fast.Foods\"])\n",
        "PCweights"
      ],
      "metadata": {
        "id": "DwkJuQUlnHuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we see that each component corresponds to the weight it puts on each of the 24 features. We can see that the first component puts about equal weight to the first six features, which were all signs of \"bad health\" (Physically.Unhealthy.Days, say). In contrast, the values of features 12 through 15 are negative, and these are all associated with \"good health habits\" (Perc.Diabetic.Screening, say). So scoring \"high\" on the first principal component (if the weighted average of features is \"high\") means that county is pretty unhealthy, ponentially due to funding or other aspects.\n",
        "\n",
        "Let's look at the scores by county:"
      ],
      "metadata": {
        "id": "-3C7KVbynoHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "principalComponentScores = pca.fit_transform(health_sc)\n",
        "pc_scores = pd.DataFrame(data = principalComponentScores, columns = ['PC1', 'PC2','PC3','PC4'])\n",
        "pc_scores"
      ],
      "metadata": {
        "id": "VT9NKRernnn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And let's add these to our data:"
      ],
      "metadata": {
        "id": "HMNcbv4Urjmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "health = health.reset_index()"
      ],
      "metadata": {
        "id": "ENc1pDxtuMOk"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "health = pd.concat([health,pc_scores], axis=1)"
      ],
      "metadata": {
        "id": "fHIDVo_CsroD"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PCA and Clusters"
      ],
      "metadata": {
        "id": "oze59bk4wTF6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To intepret the connection of our clusters and the PC-s, let's check how the clusters relate to our first principal component."
      ],
      "metadata": {
        "id": "FwYUkjxRwMqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"PC1\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a8tLaLnur77u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This implies overall that cluster 0, 2, and 3 score low on PC1. Arguably these are out \"low health\" counties, which may have to do with infrastructure and how wealthy these counties are. Indeed, let's check the proportion of children in poverty:"
      ],
      "metadata": {
        "id": "81t3kOgawqk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Perc.Children.in.Poverty\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SOyytt3wvRc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This seems to track pretty well with the first principal component.\n",
        "\n",
        "Let's look at the second principal component:"
      ],
      "metadata": {
        "id": "Jj2i7XW0xY29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"PC2\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3r4ha30Rv8ke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So we see here the picture looks quite different -- clusters 1 and 4 score low on PCA2. This appears to some extent to connect to pollution, let's check particulates:"
      ],
      "metadata": {
        "id": "PFp9jU8byHS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sns.boxplot(x = \"k_means_cl\", y = \"Avg.Daily.Particulates\", data = health)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "deXCjOP2vk3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So the counties in cluster 1 and 4 seem to have more polution."
      ],
      "metadata": {
        "id": "wtlCpPE2ygO9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To have a more formal comparison of clusters and principal components, let's prepare a scatterplot between PCs 1 and 2, and let's color them according to the clusters:"
      ],
      "metadata": {
        "id": "HO25PWQ-F-ad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "groups = health.groupby(\"k_means_cl\")\n",
        "for name, group in groups:\n",
        "    plt.plot(group.PC1, group.PC2, marker='o', linestyle='', markersize=4, label=name)\n",
        "plt.title(\"Scatter plot between PC1 and PC2\")\n",
        "plt.xlabel(\"PC1\")\n",
        "plt.ylabel(\"PC2\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "4JL11PoTE9F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So it's pretty clear that PC1 and PC2 do a good job at \"explaining\" the cluster labels."
      ],
      "metadata": {
        "id": "yXsYTDV3GtJ-"
      }
    }
  ]
}